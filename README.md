# realtime-llm-chat-app
# ğŸ§  Real-Time LLM Chat App

This project is a real-time, multi-user chat application using a local LLM via Ollama, built with:

- ğŸ”„ FastAPI + Socket.IO (backend)
- ğŸ§¾ Streamlit (frontend)
- ğŸ‹ Docker Compose (for containerization)
- ğŸ§  Ollama (for running LLMs like Mistral locally)

---

## ğŸš€ Features

- Real-time chat over Socket.IO
- Async communication between backend & frontend
- Multi-user support
- LLM-powered replies using `mistral` model via Ollama
- Containerized deployment with Docker

---

## ğŸ› ï¸ Setup Instructions

### ğŸ“¦ Prerequisites
- [Docker](https://docs.docker.com/get-docker/)
- [Ollama](https://ollama.com/) installed locally
- Model `mistral` installed locally:
  ```bash
 ollama run mistral

### 1. Install [Ollama](https://ollama.com) and pull a model:
```bash
ollama pull mistral
ollama serve
git clone https://github.com/yourusername/genai-realtime-chat.git
cd genai-realtime-chat
#docker-compose up --build

Frontend: http://localhost:8501
Backend API: http://localhost:8000

ğŸ§ª Example Use
Click "Connect to Server"

Enter your name

Type your question

Get LLM-powered response in real-time

