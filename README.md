# realtime-llm-chat-app
# ğŸ§  Real-time Chat with Local LLM (Generative AI Internship Assignment)

This is a real-time, multi-user chat system using a local LLM (via Ollama), built with FastAPI, Socket.IO, and Streamlit. Fully containerized using Docker Compose.

## ğŸš€ Features

- ğŸ”„ Real-time communication via Socket.IO
- ğŸ§‘â€ğŸ¤â€ğŸ§‘ Supports multiple users
- ğŸ§  Powered by Mistral / LLaMA 3 running on Ollama
- ğŸ§Š Easy deployment using Docker Compose
- ğŸ–¥ï¸ Clean UI using Streamlit

---

## ğŸ§± Architecture

- **Backend**: FastAPI + Socket.IO
- **Frontend**: Streamlit + Socket.IO client
- **LLM**: Local model running on Ollama (e.g., Mistral)
- **Communication**: Asynchronous, event-based

---

## âš™ï¸ Setup Instructions

### 1. Install [Ollama](https://ollama.com) and pull a model:
```bash
ollama pull mistral
ollama serve
git clone https://github.com/yourusername/genai-realtime-chat.git
cd genai-realtime-chat
#docker-compose up --build

Frontend: http://localhost:8501
Backend API: http://localhost:8000

ğŸ§ª Example Use
Click "Connect to Server"

Enter your name

Type your question

Get LLM-powered response in real-time

